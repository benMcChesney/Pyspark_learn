{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 20:14:35,813 - __main__ - INFO - running spark v3.5.4\n",
      "2025-02-12 20:14:35,815 - __main__ - INFO - loaded ini\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "import logging\n",
    "import findspark\n",
    "\n",
    "builder = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"pyspark-deltaLake-local-example\")\n",
    "        .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "findspark.init()\n",
    "findspark.add_packages('mysql:mysql-connector-java:9.2.0')\n",
    "\n",
    "# Configure logging settings\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info( f\"running spark v{spark.version}\" )\n",
    "\n",
    "import configparser\n",
    "\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the .ini file\n",
    "config.read('settings.ini')  # Replace with the path to your .ini file\n",
    "# Accessing data from the .ini file\n",
    "username = config['settings']['username']\n",
    "password = config['settings']['password']\n",
    "host = config['settings']['host']\n",
    "port = config['settings']['port']\n",
    "schema = config['database']['schema']\n",
    "logger.info('loaded ini')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|\n",
      "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|\n",
      "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|\n",
      "|  3|2013|    1|  1|   544.0|           545|     -1.0|  1004.0|          1022|    -18.0|     B6|   725| N804JB|   JFK| BQN|   183.0|    1576|   5|    45|2013-01-01 05:00:00|     JetBlue Airways|\n",
      "|  4|2013|    1|  1|   554.0|           600|     -6.0|   812.0|           837|    -25.0|     DL|   461| N668DN|   LGA| ATL|   116.0|     762|   6|     0|2013-01-01 06:00:00|Delta Air Lines Inc.|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the MySQL connection properties\n",
    "url = f\"jdbc:mysql://{host}:{port}/{schema}\"  # Replace with your MySQL server details\n",
    "# print( url ) \n",
    "properties = {\n",
    "    \"user\": username,  # Replace with your MySQL username\n",
    "    \"password\": password,  # Replace with your MySQL password\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"  # MySQL JDBC driver\n",
    "}\n",
    "\n",
    "\n",
    "# Load data from MySQL table into a PySpark DataFrame\n",
    "df = spark.read.jdbc(url=url, table=\"flights\", properties=properties)\n",
    "\n",
    "# Show the data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "| id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "|  0|2013|    1|  1|   517.0|           515|      2.0|   830.0|           819|     11.0|     UA|  1545| N14228|   EWR| IAH|   227.0|    1400|   5|    15|2013-01-01 05:00:00|United Air Lines ...|\n",
      "|  1|2013|    1|  1|   533.0|           529|      4.0|   850.0|           830|     20.0|     UA|  1714| N24211|   LGA| IAH|   227.0|    1416|   5|    29|2013-01-01 05:00:00|United Air Lines ...|\n",
      "|  2|2013|    1|  1|   542.0|           540|      2.0|   923.0|           850|     33.0|     AA|  1141| N619AA|   JFK| MIA|   160.0|    1089|   5|    40|2013-01-01 05:00:00|American Airlines...|\n",
      "+---+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "|    id|year|month|day|dep_time|sched_dep_time|dep_delay|arr_time|sched_arr_time|arr_delay|carrier|flight|tailnum|origin|dest|air_time|distance|hour|minute|          time_hour|                name|\n",
      "+------+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "|111296|2013|    2|  1|   456.0|           500|     -4.0|   652.0|           648|      4.0|     US|  1117| N197UW|   EWR| CLT|    98.0|     529|   5|     0|2013-02-01 05:00:00|     US Airways Inc.|\n",
      "|111299|2013|    2|  1|   532.0|           540|     -8.0|  1007.0|          1017|    -10.0|     B6|   725| N554JB|   JFK| BQN|   195.0|    1576|   5|    40|2013-02-01 05:00:00|     JetBlue Airways|\n",
      "|111297|2013|    2|  1|   520.0|           525|     -5.0|   816.0|           820|     -4.0|     UA|  1018| N24211|   EWR| IAH|   209.0|    1400|   5|    25|2013-02-01 05:00:00|United Air Lines ...|\n",
      "+------+----+-----+---+--------+--------------+---------+--------+--------------+---------+-------+------+-------+------+----+--------+--------+----+------+-------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView( \"flights_merge\" )\n",
    "\n",
    "# setup two dataframes to \"merge\"\n",
    "# Show the result of the SQL query\n",
    "query_sql = \"SELECT * FROM flights_merge WHERE month = 1\"\n",
    "first_month = spark.sql(query_sql)\n",
    "first_month.createOrReplaceTempView( \"flights_Jan\")\n",
    "first_month.show(3)\n",
    "\n",
    "# df.show(5)\n",
    "query_sql_2 = \"SELECT * FROM flights_merge WHERE month <= 2 order by month desc\"\n",
    "second_month = spark.sql(query_sql_2)\n",
    "second_month.createOrReplaceTempView( \"flights_Feb\")\n",
    "second_month.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 20:34:16,093 - __main__ - INFO - delta exists! skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|max(month)|max(day)|\n",
      "+----------+--------+\n",
      "|         2|      31|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# one time run \n",
    "delta_path = \"./bronze/flights\"\n",
    "delta_table_exists = DeltaTable.isDeltaTable(spark, delta_path)\n",
    "\n",
    "if delta_table_exists:\n",
    "    logger.info( 'delta exists! skipping...')\n",
    "    # Load the Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # Convert the Delta table to a DataFrame\n",
    "    delta_df = delta_table.toDF()\n",
    "    delta_df.createOrReplaceTempView( \"delta_flights\")\n",
    "    query_sql = \"SELECT max(month) , max(day) FROM delta_flights \"\n",
    "    delta_results = spark.sql(query_sql)\n",
    "    \n",
    "    delta_results.show(3)\n",
    "\n",
    "\n",
    "else:\n",
    "    logger.info( f'delta table {delta_path} does not exist, creating')\n",
    "    # If the Delta table does not exist, create one by writing out the current aggregation\n",
    "    first_month.write.format('delta').mode('overwrite').save( delta_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 20:55:30,414 - __main__ - INFO - merge compelted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delta_path = \"./bronze/flights\"\n",
    "delta_table_exists = DeltaTable.isDeltaTable(spark, delta_path)\n",
    "\n",
    "if delta_table_exists:\n",
    "\n",
    "    # Load the Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "    # Perform UPSERT (MERGE)\n",
    "    # Define the condition for the merge with multiple columns\n",
    "    merge_condition = \"\"\"source.month = target.month \n",
    "    AND source.day = target.day\n",
    "    AND source.flight = target.flight\n",
    "    AND source.tailnum = target.tailnum\n",
    "    AND source.carrier = target.carrier\"\"\"\n",
    "\n",
    "    # Perform the merge operation\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        second_month.alias(\"source\"),\n",
    "        merge_condition\n",
    "    ).whenMatchedUpdateAll(\n",
    "    ).whenNotMatchedInsertAll(\n",
    "    ).execute() \n",
    "\n",
    "    logger.info( \"merge completed\")\n",
    "\n",
    "else:\n",
    "\n",
    "    logger.error( f\" delta table does not exist should have been made in previous cell\" )  \n",
    "    # If the Delta table does not exist, create one by writing out the current aggregation\n",
    "    # df.write.format('delta').mode('overwrite').save( delta_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|max(month)|max(day)|\n",
      "+----------+--------+\n",
      "|         2|      31|\n",
      "+----------+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   27004|\n",
      "|   24951|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table_exists = DeltaTable.isDeltaTable(spark, delta_path)\n",
    "\n",
    "if delta_table_exists:\n",
    "\n",
    "    delta_table_df = delta_table.toDF().createOrReplaceTempView( \"delta_flights\" )\n",
    "    query_sql = \"SELECT max(month) , max(day) FROM delta_flights \"\n",
    "    delta_results = spark.sql(query_sql)\n",
    "    delta_results.show()\n",
    "\n",
    "    query_sql = \"SELECT count(*) FROM delta_flights group by month\"\n",
    "    delta_results = spark.sql(query_sql)\n",
    "    delta_results.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
